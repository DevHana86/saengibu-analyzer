import streamlit as st
import pdfplumber
import re
import tempfile
import os
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
from sentence_transformers import SentenceTransformer, util
import pandas as pd
import altair as alt

def clean_semester_bracket(line):
    return re.sub(r'\((1|2)í•™ê¸°\)', '', line).strip()

def clean_hours_bracket(line):
    return re.sub(r'\(\d+\s*ì‹œê°„\)', '', line).strip()

def extract_and_clean_pdf(pdf_path):
    remove_startswith = ["ì¶©ë‚¨ì‚¼ì„±ê³ ë“±í•™êµ"]
    remove_patterns = [
        r'êµ¬\s*ë¶„.*ëª…ì¹­.*ë²ˆí˜¸.*ì·¨ë“ì—°ì›”ì¼.*ë°œê¸‰ê¸°ê´€',
        r'í•™ë…„.*í•™ê¸°.*ì„¸ë¶„ë¥˜.*ì´ìˆ˜ì‹œê°„.*ì›ì ìˆ˜.*ì„±ì·¨ë„.*ë¹„ê³ ',
        r'ì¼ì.*ì¥ì†Œ.*í™œë™ë‚´ìš©.*ì‹œê°„.*ëˆ„ê³„ì‹œê°„'
    ]

    with pdfplumber.open(pdf_path) as pdf:
        text = ""
        for page in pdf.pages[1:]:
            page_text = page.extract_text()
            if page_text:
                text += page_text + "\n"

    cleaned_lines = []
    for line in text.splitlines():
        stripped_line = line.strip()
        if len(stripped_line) <= 20:
            continue
        if any(stripped_line.startswith(word) for word in remove_startswith):
            continue
        if any(re.search(pattern, stripped_line) for pattern in remove_patterns):
            continue
        if re.search(r'\d+/\d+', stripped_line) or re.search(r'[ABC]\(\d+(\.\d+)?\)', stripped_line):
            continue
        if re.match(r'^\d', stripped_line):
            continue

        cleaned_line = clean_semester_bracket(stripped_line)
        cleaned_line = clean_hours_bracket(cleaned_line)
        if cleaned_line:
            cleaned_lines.append(cleaned_line)

    return "\n".join(cleaned_lines)

def load_keyword_lines(filepath):
    with open(filepath, 'r', encoding='utf-8') as f:
        return [line.strip() for line in f if line.strip()]

@st.cache_resource
def load_sbert_model():
    return SentenceTransformer('snunlp/KR-SBERT-V40K-klueNLI-augSTS')

def ë¶„ì„_ì „ì²´(pdf_path, categories):
    cleaned_text = extract_and_clean_pdf(pdf_path)

    with st.spinner("SBERT ëª¨ë¸ ë¡œë”© ì¤‘..."):
        model = load_sbert_model()

    sentences = re.split(r'[.!?]\s*|\n', cleaned_text)
    sentences = [s.strip() for s in sentences if len(s.strip()) > 10]
    sentence_embeddings = model.encode(sentences)

    chart_data_all = {}

    for category, lines in categories.items():
        documents = [line.split(':', 1)[1] if ':' in line else line for line in lines]
        labels = [line.split(':', 1)[0] if ':' in line else line[:10] for line in lines]

        keyword_embeddings = model.encode(documents)
        sim_matrix = util.cos_sim(keyword_embeddings, sentence_embeddings).cpu().numpy()

        keyword_evidence = {}
        for i, label in enumerate(labels):
            sentence_scores = list(zip(sentences, sim_matrix[i]))
            top_sentences = sorted(sentence_scores, key=lambda x: x[1], reverse=True)[:3]
            keyword_evidence[label] = top_sentences

        main_embedding = model.encode([cleaned_text])
        sbert_sims = util.cos_sim(main_embedding, keyword_embeddings)[0].cpu().numpy().tolist()

        tfidf_vectorizer = TfidfVectorizer()
        tfidf_matrix = tfidf_vectorizer.fit_transform([cleaned_text] + documents)
        tfidf_scores = cosine_similarity(tfidf_matrix[0:1], tfidf_matrix[1:])[0]

        hybrid_scores = [tfidf_score * (1 + sbert_sim) for tfidf_score, sbert_sim in zip(tfidf_scores, sbert_sims)]
        df = pd.DataFrame({
            'í‚¤ì›Œë“œ': labels,
            'TF-IDF': tfidf_scores,
            'SBERT': sbert_sims,
            'ì˜ë¯¸ ê°€ì¤‘ ì ìˆ˜': hybrid_scores,
            'ê·¼ê±° ë¬¸ì¥': [keyword_evidence[label] for label in labels]
        })

        df_for_chart = df.drop(columns=['ê·¼ê±° ë¬¸ì¥'])
        top_df = df_for_chart.sort_values('ì˜ë¯¸ ê°€ì¤‘ ì ìˆ˜', ascending=False).head(5)

        chart_data_all[category] = {
            'chart_df': top_df,
            'evidence': keyword_evidence
        }


    return cleaned_text, chart_data_all

st.set_page_config(page_title="ìƒê¸°ë¶€ í‚¤ì›Œë“œ ë¶„ì„ TOOL", layout="wide")

if 'analyzed' not in st.session_state:
    st.session_state.analyzed = False
if 'page' not in st.session_state:
    st.session_state.page = 'home'
if 'analyze_clicked' not in st.session_state:
    st.session_state.analyze_clicked = False

st.title("ğŸš€ ìƒê¸°ë¶€ í‚¤ì›Œë“œ ë¶„ì„ TOOL")

uploaded_file = st.file_uploader("ìƒê¸°ë¶€ PDF ì—…ë¡œë“œ", type=["pdf"])

if uploaded_file:
    temp_pdf = tempfile.NamedTemporaryFile(delete=False, suffix=".pdf")
    temp_pdf.write(uploaded_file.read())
    temp_pdf.close()

    try:
        categories = {
            'ì—­ëŸ‰': load_keyword_lines('ì—­ëŸ‰.txt'),
            'íƒœë„': load_keyword_lines('íƒœë„.txt'),
            'ê´€ì‹¬ì‚¬': load_keyword_lines('ê´€ì‹¬ì‚¬.txt')
        }
    except Exception:
        st.error("âš ï¸ í‚¤ì›Œë“œ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
        os.unlink(temp_pdf.name)
        st.stop()

    if not st.session_state.analyzed and not st.session_state.analyze_clicked:
        if st.button("ğŸ” ë¶„ì„ ì‹œì‘"):
            st.session_state.analyze_clicked = True
            st.rerun()

    if st.session_state.analyze_clicked and not st.session_state.analyzed:
        with st.spinner("ë¶„ì„ ì¤‘..."):
            cleaned_text, chart_data_all = ë¶„ì„_ì „ì²´(temp_pdf.name, categories)
            st.session_state.cleaned_text = cleaned_text
            st.session_state.chart_data_all = chart_data_all
            st.session_state.analyzed = True
            st.session_state.analyze_clicked = False
            st.success("âœ… ë¶„ì„ ì™„ë£Œ.")

    if st.session_state.analyzed:
        if st.button("ğŸ“Š ê²°ê³¼ í™•ì¸í•˜ê¸°"):
            st.session_state.page = 'result'

    os.unlink(temp_pdf.name)

if st.session_state.get('page') == 'result':
    st.title("ğŸ“Š ë¶„ì„ ê²°ê³¼")

    chart_data_all = st.session_state.chart_data_all
    cleaned_text = st.session_state.cleaned_text

    tab1, tab2 = st.tabs(["ìƒìœ„ í‚¤ì›Œë“œ ìš”ì•½", "ì˜ë¯¸ ê°€ì¤‘ ì ìˆ˜ ê·¸ë˜í”„"])

    with tab1:
        st.write("### ğŸ·ï¸ ìƒìœ„ í‚¤ì›Œë“œ + ê·¼ê±° ë¬¸ì¥ ë³´ê¸°")
        cols = st.columns(3)
        for i, (category, data) in enumerate(chart_data_all.items()):
            df = data['chart_df']
            evidence_dict = data['evidence']

            with cols[i]:
                st.markdown(f"<h3 style='color:#FF4B4B'>{category}</h3>", unsafe_allow_html=True)
                for _, row in df.iterrows():
                    with st.expander(f"ğŸ” {row['í‚¤ì›Œë“œ']}"):
                        for sent, score in evidence_dict[row['í‚¤ì›Œë“œ']]:
                            st.markdown(f"- {sent}  \n<span style='color:gray'>ìœ ì‚¬ë„: {score:.3f}</span>", unsafe_allow_html=True)

    with tab2:
        st.write("### ğŸ“ˆ ì˜ë¯¸ ê°€ì¤‘ ì ìˆ˜ ê·¸ë˜í”„")
        category_colors = {
            'ì—­ëŸ‰': '#FF4B4B',
            'íƒœë„': '#4B7BEC',
            'ê´€ì‹¬ì‚¬': '#FFA41B'
        }

        cols = st.columns(3)
        for i, (category, data) in enumerate(chart_data_all.items()):
            df = data['chart_df']

            with cols[i]:
                st.markdown(f"##### {category}")
                chart = alt.Chart(df).mark_bar(
                    cornerRadiusTopLeft=10,
                    cornerRadiusTopRight=10,
                    color=category_colors.get(category, '#888888')
                ).encode(
                    x=alt.X('í‚¤ì›Œë“œ', sort='-y'),
                    y=alt.Y('ì˜ë¯¸ ê°€ì¤‘ ì ìˆ˜'),
                    tooltip=['í‚¤ì›Œë“œ', 'ì˜ë¯¸ ê°€ì¤‘ ì ìˆ˜', 'TF-IDF', 'SBERT']
                ).properties(width=300, height=300)
                st.altair_chart(chart, use_container_width=True)

    st.divider()
    st.write("### ğŸ“¥ ìƒê¸°ë¶€ í…ìŠ¤íŠ¸ ì¶”ì¶œë³¸ ë‹¤ìš´ë¡œë“œ")
    st.download_button(
        label="ìƒê¸°ë¶€_ì¶”ì¶œë³¸.txt ë‹¤ìš´ë¡œë“œ",
        data=cleaned_text,
        file_name="ìƒê¸°ë¶€_ì¶”ì¶œë³¸.txt",
        mime="text/plain"
    )

    st.divider()
    if st.button("ğŸ  ë‹¤ì‹œ ì‹œì‘í•˜ê¸°"):
        st.session_state.page = 'home'
        st.session_state.analyzed = False